{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e20047e9",
   "metadata": {},
   "source": [
    "<h1> LARN SERVER </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499158a4",
   "metadata": {},
   "source": [
    "<h3> Importing Libraries </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e71fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import face_recognition\n",
    "import os\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from fer import Video, FER\n",
    "import sys\n",
    "import dlib\n",
    "import pynormalize\n",
    "from moviepy.editor import *\n",
    "import librosa\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "import pickle\n",
    "from scipy.spatial import distance as dist\n",
    "from imutils.video import FileVideoStream, VideoStream\n",
    "from imutils import face_utils,resize\n",
    "import time\n",
    "import LengthCalculator as lc          #From LengthCalculator.py\n",
    "from Emotions import Emotions as em    #From Emotions.py\n",
    "import threading as th\n",
    "import socket\n",
    "import tqdm\n",
    "from joblib import load\n",
    "import xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02cb47a",
   "metadata": {},
   "source": [
    "<h3> Importing Pre-Trained Models </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604377d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "head_model = tf.keras.models.load_model('head_movement_model')\n",
    "eye_model = pickle.load(open('EyeBlinkModel_saved.sav', 'rb'))\n",
    "speech_model = tf.keras.models.load_model(\"my_model\") # Speech model\"\n",
    "expression_model = pickle.load(open('facialexprknn.sav', 'rb'))\n",
    "xgb_model_loaded = pickle.load(open(\"micro_xgb.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6b23d1",
   "metadata": {},
   "source": [
    "<h3> Head Movement Analysis </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0c2742",
   "metadata": {},
   "outputs": [],
   "source": [
    "def head_analysis(path, res, ind):\n",
    "    video_name = os.path.basename(path)\n",
    "    file_name = f'hm_video_{video_name}.txt'\n",
    "    f = open(file_name,'w+')\n",
    "    Camera = cv2.VideoCapture(path)\n",
    "    _,frame = Camera.read()\n",
    "\n",
    "    frameRGB = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    box = face_recognition.face_locations(frameRGB)          \n",
    "\n",
    "    cx_ = (box[0][3] + box[0][1])/2\n",
    "    cy_ = (box[0][3] + box[0][1])/2\n",
    "    cx = cx_\n",
    "    cy = cy_\n",
    "\n",
    "    MIN_MOVE=10\n",
    "    while True:\n",
    "        ret,frame = Camera.read()\n",
    "        if ret == True:\n",
    "            frameRGB = cv2.cvtColor(frame,cv2.COLOR_BGR2RGB)\n",
    "            box = face_recognition.face_locations(frameRGB)          \n",
    "\n",
    "            if ( box!= [] ):\n",
    "                cx = (box[0][3] + box[0][1])/2\n",
    "                cy = (box[0][0] + box[0][2])/2\n",
    "                cv2.rectangle( frame ,(box[0][3],box[0][2]) , (box[0][1],box[0][0]) , (0,0,255) , 2 )\n",
    "\n",
    "                if abs(cx-cx_) > abs(cy-cy_):\n",
    "                    if cx - cx_ > MIN_MOVE:\n",
    "                        f.write('LEFT\\n')\n",
    "                    elif cx - cx_ < -MIN_MOVE:\n",
    "                        f.write('RIGHT\\n')\n",
    "                else:\n",
    "                    if cy - cy_ > MIN_MOVE:\n",
    "                        f.write('DOWN\\n')\n",
    "                    elif cy - cy_ < -MIN_MOVE:\n",
    "                        f.write('UP\\n')\n",
    "            key = cv2.waitKey(30)\n",
    "            cx_ = cx\n",
    "            cy_ = cy\n",
    "            if key == 27:\n",
    "                break\n",
    "        else:\n",
    "            break\n",
    "    f.close()\n",
    "    \n",
    "    col_headers =['UP', 'DOWN', 'LEFT', 'RIGHT', 'LEFT_RIGHT', 'LEFT_UP', 'LEFT_DOWN', 'RIGHT_LEFT', 'RIGHT_UP', 'RIGHT_DOWN', 'UP_LEFT', 'UP_RIGHT', 'UP_DOWN', 'DOWN_LEFT', 'DOWN_RIGHT', 'DOWN_UP']\n",
    "    df_head = pd.DataFrame(columns=col_headers)\n",
    "    direction_count = dict()\n",
    "    for col in col_headers:\n",
    "        direction_count[col] = 0\n",
    "\n",
    "    with open(file_name) as f:\n",
    "        data = f.readlines()\n",
    "\n",
    "    line1_p = 0\n",
    "    line2_p = 1\n",
    "\n",
    "    while line2_p != len(data):\n",
    "        line1 = data[line1_p].rstrip()\n",
    "        line2 = data[line2_p].rstrip()\n",
    "        if line1 == line2:\n",
    "            direction_count[line1] += 1\n",
    "        else:\n",
    "            temp = line1 + '_' + line2\n",
    "            direction_count[temp] += 1\n",
    "        line1_p += 1\n",
    "        line2_p += 1\n",
    "    df_head = df_head.append(direction_count, ignore_index=True)\n",
    "    \n",
    "    dataset = df_head.to_numpy()\n",
    "    X = dataset[:,0:16]\n",
    "    X = np.asarray(X).astype('float32')\n",
    "    head_pred = head_model.predict(X)\n",
    "    res[ind] = (head_pred > 0.5).astype(int)[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031af9c5",
   "metadata": {},
   "source": [
    "<h3> Eye Blink Analysis </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b386775",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eye_aspect_ratio(eye):\n",
    "    A = dist.euclidean(eye[1], eye[5])\n",
    "    B = dist.euclidean(eye[2], eye[4])\n",
    "    C = dist.euclidean(eye[0], eye[3])\n",
    "    ear = (A + B) / (2.0 * C)\n",
    "    return ear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cc6de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eye_analysis(path, res, ind):\n",
    "    p = 'shape_predictor_68_face_landmarks.dat'\n",
    "    args = {'shape_predictor':p, 'video':path}\n",
    "\n",
    "    EYE_AR_THRESH = 0.3\n",
    "    EYE_AR_CONSEC_FRAMES = 3\n",
    "\n",
    "    COUNTER = 0\n",
    "    TOTAL = 0\n",
    "    FRAME = 0\n",
    "\n",
    "    detector = dlib.get_frontal_face_detector()\n",
    "    predictor = dlib.shape_predictor(args[\"shape_predictor\"])\n",
    "\n",
    "    (lStart, lEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\"left_eye\"]\n",
    "    (rStart, rEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\"right_eye\"]\n",
    "    vs = FileVideoStream(args[\"video\"]).start()\n",
    "    fileStream = True\n",
    "    time.sleep(1.0)\n",
    "\n",
    "    while True:\n",
    "        if fileStream and not vs.more():\n",
    "            break\n",
    "        frame = vs.read()\n",
    "        if frame is None:\n",
    "            break\n",
    "        frame = resize(frame, width=450)\n",
    "        FRAME += 1\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        rects = detector(gray, 0)\n",
    "        for rect in rects:\n",
    "            shape = predictor(gray, rect)\n",
    "            shape = face_utils.shape_to_np(shape)\n",
    "            leftEye = shape[lStart:lEnd]\n",
    "            rightEye = shape[rStart:rEnd]\n",
    "            leftEAR = eye_aspect_ratio(leftEye)\n",
    "            rightEAR = eye_aspect_ratio(rightEye)\n",
    "            ear = (leftEAR + rightEAR) / 2.0\n",
    "            leftEyeHull = cv2.convexHull(leftEye)\n",
    "            rightEyeHull = cv2.convexHull(rightEye)\n",
    "            cv2.drawContours(frame, [leftEyeHull], -1, (0, 255, 0), 1)\n",
    "            cv2.drawContours(frame, [rightEyeHull], -1, (0, 255, 0), 1)\n",
    "            \n",
    "            if ear < EYE_AR_THRESH:\n",
    "                COUNTER += 1\n",
    "            else:\n",
    "                if COUNTER >= EYE_AR_CONSEC_FRAMES:\n",
    "                    TOTAL += 1\n",
    "                COUNTER = 0\n",
    "\n",
    "            cv2.putText(frame, \"Blinks: {}\".format(TOTAL), (10, 30),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "            cv2.putText(frame, \"EAR: {:.2f}\".format(ear), (300, 30),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "        if key == ord(\"q\"):\n",
    "            break\n",
    "    X = (TOTAL/FRAME)*25*60\n",
    "    cv2.destroyAllWindows()\n",
    "    vs.stop()\n",
    "    X = np.array([X])\n",
    "    X = X.reshape(1,-1)\n",
    "    eye_pred = 1 if eye_model.predict(X) > 0.5 else 0\n",
    "    res[ind] = eye_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc39c3e0",
   "metadata": {},
   "source": [
    "<h3> Face Expresssion Analysis </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c61583",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores(path):\n",
    "    location_videofile = path\n",
    "    face_detector = FER(mtcnn=True)\n",
    "    input_video = Video(location_videofile)\n",
    "    processing_data = input_video.analyze(face_detector, display=False)\n",
    "    vid_df = input_video.to_pandas(processing_data)\n",
    "    vid_df = input_video.get_first_face(vid_df)\n",
    "    vid_df = input_video.get_emotions(vid_df)\n",
    "    angry = sum(vid_df.angry)\n",
    "    disgust = sum(vid_df.disgust)\n",
    "    fear = sum(vid_df.fear)\n",
    "    happy = sum(vid_df.happy)\n",
    "    sad = sum(vid_df.sad)\n",
    "    surprise = sum(vid_df.surprise)\n",
    "    neutral = sum(vid_df.neutral)\n",
    "    emotions = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "    emotions_values = [angry, disgust, fear, happy, sad, surprise, neutral]\n",
    "    score_comparisons = pd.DataFrame(emotions, columns = ['Human Emotions'])\n",
    "    score_comparisons['Emotion Value from the Video'] = emotions_values\n",
    "    return score_comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54dd9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expression_analysis(path, res, ind):\n",
    "    score = get_scores(path)\n",
    "    scores = score['Emotion Value from the Video'][:5]\n",
    "    scores = list(score['Emotion Value from the Video'][:5])\n",
    "    facial_pred = expression_model.predict([scores])\n",
    "    res[ind] =(facial_pred[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296c398f",
   "metadata": {},
   "source": [
    "<h3> Speech Analysis </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb54551",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(path):\n",
    "    # Sets the name to be the path to where the file is in my computer\n",
    "    # Loads the audio file as a floating point time series and assigns the default sample rate\n",
    "    # Sample rate is set to 22050 by default\n",
    "    X, sample_rate = librosa.load(path, res_type='kaiser_fast')\n",
    "    # Generate Mel-frequency cepstral coefficients (MFCCs) from a time series \n",
    "    mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T,axis=0)\n",
    "    # Generates a Short-time Fourier transform (STFT) to use in the chroma_stft\n",
    "    stft = np.abs(librosa.stft(X))\n",
    "    # Computes a chromagram from a waveform or power spectrogram.\n",
    "    chroma = np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T,axis=0)\n",
    "    # Computes a mel-scaled spectrogram.\n",
    "    mel = np.mean(librosa.feature.melspectrogram(X, sr=sample_rate).T,axis=0)\n",
    "    # Computes spectral contrast\n",
    "    contrast = np.mean(librosa.feature.spectral_contrast(S=stft, sr=sample_rate).T,axis=0)\n",
    "    # Computes the tonal centroid features (tonnetz)\n",
    "    tonnetz = np.mean(librosa.feature.tonnetz(y=librosa.effects.harmonic(X),\n",
    "    sr=sample_rate).T,axis=0)\n",
    "    return mfccs, chroma, mel, contrast, tonnetz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866efb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def speech_analysis(path, res, ind):\n",
    "    video_name = os.path.basename(path)\n",
    "    audioclip = AudioFileClip(path)\n",
    "    audioclip.write_audiofile(video_name+\".wav\")\n",
    "    features = extract_features(video_name+\".wav\")\n",
    "    features = np.concatenate(features)\n",
    "    features = features.reshape(1,193)\n",
    "    pred = speech_model.predict(features)\n",
    "    speech_pred = np.argmax(pred, axis=1)\n",
    "    res[ind] = speech_pred[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2243f8df",
   "metadata": {},
   "source": [
    "<h3> Facial Micro Expression Model </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9067aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_gamma(image, gamma=1.0):\n",
    "    invGamma = 1.0 / gamma\n",
    "    table = np.array([((i / 255.0) ** invGamma) * 255\n",
    "        for i in np.arange(0, 256)]).astype(\"uint8\")\n",
    "    return cv2.LUT(image, table)\n",
    "\n",
    "def draw_landmarks(image,landmarks):\n",
    "    coords=[]\n",
    "    for index in range(68):\n",
    "        coords.append((landmarks.part(index).x,landmarks.part(index).y))\n",
    "    coords=np.array(coords,np.int32)\n",
    "    for elems in coords:\n",
    "        cv2.circle(image,(elems[0],elems[1]), 1, (0,255,0),-1)\n",
    "    return image,coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a49026",
   "metadata": {},
   "outputs": [],
   "source": [
    "def micro_expression_analysis(path, final_res, index):\n",
    "    font = cv2.FONT_HERSHEY_PLAIN\n",
    "    cap = cv2.VideoCapture(path) \n",
    "    detector = dlib.get_frontal_face_detector()\n",
    "    predictor = dlib.shape_predictor('shape_predictor_68_face_landmarks.dat')\n",
    "    emot=em()\n",
    "    res=[]\n",
    "    while True: \n",
    "        ret, frame = cap.read()\n",
    "        if ret == True:\n",
    "            frame=adjust_gamma(frame,1.7)\n",
    "            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "            faces = detector(gray)\n",
    "            for face in faces:\n",
    "                if len(faces)==1:\n",
    "                    landmarks = predictor(gray, face)\n",
    "                    frame,coords=draw_landmarks(frame,landmarks)\n",
    "                    lis=emot.process_data(lc.printing(frame,coords))\n",
    "                    if(lis!=None):\n",
    "                        res.append(lis)\n",
    "        else:\n",
    "            break\n",
    "    cap.release()\n",
    "    result = []\n",
    "    for i in res:\n",
    "        for j in i:\n",
    "            dic = {\"Left mouth corner\":0,\"Right mouth corner\":0,\"Left eyebrow\":0,\"Right eyebrow\":0,\"Left eye open\":0,\"Left eye close\":0,\"Right eye open\":0,\"Right eye close\":0}\n",
    "            if dic.__contains__(j):\n",
    "                dic[j]=1\n",
    "        result.append(dic)\n",
    "    df1=pd.DataFrame(result)\n",
    "    x=xgb_model_loaded.predict(df1.values)\n",
    "    micro_pred = np.bincount(x).argmax()\n",
    "    final_res[index] = micro_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3adbdda7",
   "metadata": {},
   "source": [
    "<h3> Combination of Model </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867d7419",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combination(path):\n",
    "    acc_scores = [0.6, 0.8, 0.6, 0.8, 0.628]\n",
    "    pred_scores = [0, 0, 0, 0, 0]\n",
    "    t0 = th.Thread(target=eye_analysis, args=(path, pred_scores, 0))\n",
    "    t1 = th.Thread(target=speech_analysis, args=(path, pred_scores, 1))\n",
    "    t2 = th.Thread(target=expression_analysis, args=(path, pred_scores, 2))\n",
    "    t3 = th.Thread(target=head_analysis, args=(path, pred_scores, 3))\n",
    "    t4 = th.Thread(target=micro_expression_analysis, args=(path, pred_scores, 4))\n",
    "    \n",
    "    t0.start()\n",
    "    t1.start()\n",
    "    t2.start()\n",
    "    t3.start()\n",
    "    t4.start()\n",
    "    \n",
    "    t0.join()\n",
    "    t1.join()\n",
    "    t2.join()\n",
    "    t3.join()\n",
    "    t4.join()\n",
    "    \n",
    "    final_pred = np.dot(np.array(acc_scores), np.array(pred_scores)) / 3.428\n",
    "    if final_pred > 0.5:\n",
    "        return 'Truth'\n",
    "    else: \n",
    "        return 'Lie'   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98acf515",
   "metadata": {},
   "source": [
    "<h3> Server </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458176cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device's IP address\n",
    "SERVER_HOST = \"127.0.0.1\"\n",
    "SERVER_PORT = 8080\n",
    "\n",
    "# receive 4096 bytes each time\n",
    "BUFFER_SIZE = 4096\n",
    "SEPARATOR = \"<SEPARATOR>\"\n",
    "\n",
    "# create the server socket\n",
    "# TCP socket\n",
    "s = socket.socket()\n",
    "\n",
    "# bind the socket to our local address\n",
    "s.bind((SERVER_HOST, SERVER_PORT))\n",
    "\n",
    "# enabling our server to accept connections\n",
    "# 10 here is the number of unaccepted connections that\n",
    "# the system will allow before refusing new connections\n",
    "while True:\n",
    "    s.listen(10)\n",
    "    print(f\"[*] Listening as {SERVER_HOST}:{SERVER_PORT}\")\n",
    "    \n",
    "    # accept connection if there is any\n",
    "    client_socket, address = s.accept() \n",
    "    \n",
    "    # if below code is executed, that means the sender is connected\n",
    "    print(f\"[+] {address} is connected.\")\n",
    "\n",
    "    # receive the file infos\n",
    "    # receive using client socket, not server socket\n",
    "    received = client_socket.recv(BUFFER_SIZE).decode()\n",
    "    client_addr, filename, filesize = received.split(SEPARATOR)\n",
    "    \n",
    "    # remove absolute path if there is\n",
    "    filename = os.path.basename(filename)\n",
    "    \n",
    "    # convert to integer\n",
    "    filesize = int(filesize)\n",
    "    \n",
    "    # start receiving the file from the socket\n",
    "    # and writing to the file stream\n",
    "    with open(filename, \"wb\") as f:\n",
    "        while True:\n",
    "            # read 4096 bytes from the socket (receive)\n",
    "            bytes_read = client_socket.recv(BUFFER_SIZE)\n",
    "            \n",
    "            if not bytes_read:    \n",
    "                # nothing is received\n",
    "                # file transmitting is done\n",
    "                break\n",
    "            \n",
    "            # write to the file the bytes we just received\n",
    "            f.write(bytes_read)\n",
    "    \n",
    "    # close the client socket\n",
    "    client_socket.close()\n",
    "    \n",
    "    #run the combination model and obtain the result\n",
    "    result = combination(filename)\n",
    "    \n",
    "    #create an UDP socket to send the result to the client\n",
    "    clientSock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n",
    "    \n",
    "    #encode the result and send it to the client\n",
    "    clientSock.sendto(result.encode(), (client_addr, 8040))\n",
    "    \n",
    "# close the server socket\n",
    "s.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb51c61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf2a538",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
